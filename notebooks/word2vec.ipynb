{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取文本特征\n",
    "词向量 + 字向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:58:12.943155Z",
     "start_time": "2020-06-12T10:58:12.540541Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "import zhconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T01:01:34.264510Z",
     "start_time": "2020-06-11T01:01:34.262931Z"
    }
   },
   "source": [
    "## 保存分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:59:47.986421Z",
     "start_time": "2020-06-12T10:59:47.980457Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_cut_word_rst(file_path):\n",
    "    data = pd.read_csv(file_path + \".csv\", usecols=['content'])\n",
    "    with open(file_path + \"_words_list.txt\", 'w') as f_w:\n",
    "        for content in data['content'].values:\n",
    "            content = zhconv.convert(content.strip(), 'zh-cn')\n",
    "            content = list(\n",
    "                filter(lambda x: len(x.strip()) > 0, list(jieba.cut(content))))\n",
    "            f_w.write(' '.join(content) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:02:10.194540Z",
     "start_time": "2020-06-12T10:59:51.563664Z"
    }
   },
   "outputs": [],
   "source": [
    "for file_path in [\n",
    "    './datasets/trainingset',\n",
    "    './datasets/validationset',\n",
    "    './datasets/testa'\n",
    "]:\n",
    "    save_cut_word_rst(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:02:20.778171Z",
     "start_time": "2020-06-12T11:02:20.158921Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./datasets/all_dataset_word_list.txt\", 'w') as f_w:\n",
    "    for file_path in [\n",
    "            './datasets/trainingset',\n",
    "            './datasets/validationset',\n",
    "            './datasets/testa',\n",
    "    ]:\n",
    "        with open(file_path + '_words_list.txt') as f_r:\n",
    "            for line in f_r:\n",
    "                f_w.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T01:02:43.243853Z",
     "start_time": "2020-06-11T01:02:43.238794Z"
    }
   },
   "source": [
    "## 词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:03:57.742781Z",
     "start_time": "2020-06-12T11:03:57.736712Z"
    }
   },
   "outputs": [],
   "source": [
    "# 停用词\n",
    "\n",
    "def get_stop_word_set(only_punctuation=False):\n",
    "    words_set = set()\n",
    "    fname = './datasets/哈工大停用标点表.txt' if only_punctuation else \\\n",
    "        './datasets/哈工大停用词表扩展.txt'\n",
    "    with open(fname) as f_r:\n",
    "        for line in f_r:\n",
    "            words_set |= set(line.strip())\n",
    "    if only_punctuation:\n",
    "        words_set |= set([' '])\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:03:58.868776Z",
     "start_time": "2020-06-12T11:03:58.863331Z"
    }
   },
   "outputs": [],
   "source": [
    "class MySentence:\n",
    "    def __init__(self, dirname, filter_ws):\n",
    "        self.dirname = dirname\n",
    "        self.filter_ws = filter_ws\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.dirname):\n",
    "            yield list(\n",
    "                filter(lambda x: x not in self.filter_ws,\n",
    "                       line.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:07:19.925533Z",
     "start_time": "2020-06-12T11:04:06.748314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69164128.0 127527\n"
     ]
    }
   ],
   "source": [
    "sentences = MySentence(\"./datasets/all_dataset_word_list.txt\",\n",
    "                       get_stop_word_set(only_punctuation=True))\n",
    "model = Word2Vec(sentences,\n",
    "                 sg=1,\n",
    "                 size=100,\n",
    "                 compute_loss=True,\n",
    "                 window=5,\n",
    "                 workers=8,\n",
    "                 iter=8,\n",
    "                 min_count=2)\n",
    "print(model.get_latest_training_loss(), len(model.wv.vocab))\n",
    "model.save(\"./saved/word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:07:35.240953Z",
     "start_time": "2020-06-12T11:07:35.234474Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存所有单字，到同一文件，追加模式 'a'\n",
    "\n",
    "\n",
    "def save_char_content(save_path, fpath, stop_word_set):\n",
    "    data = pd.read_csv(fpath, usecols=['content'])\n",
    "    with open(save_path, 'a') as f_w:\n",
    "        for con in data['content'].values:\n",
    "            f_w.write(\" \".join(\n",
    "                list(\n",
    "                    filter(\n",
    "                        lambda x: x not in stop_word_set and len(x.strip()) >\n",
    "                        0, zhconv.convert(con, 'zh-cn')))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:07:36.061342Z",
     "start_time": "2020-06-12T11:07:36.054838Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存单字到对应的，单个文件\n",
    "\n",
    "\n",
    "def save_char_content_single(fpath, stop_word_set):\n",
    "    data = pd.read_csv(fpath, usecols=['content'])\n",
    "    fpath = fpath[:fpath.rfind('.')] + '_char_list.txt'\n",
    "    print(fpath)\n",
    "    with open(fpath, 'w') as f_w:\n",
    "        for con in data['content'].values:\n",
    "            f_w.write(' '.join(\n",
    "                list(\n",
    "                    filter(\n",
    "                        lambda x: x not in stop_word_set and len(x.strip()) >\n",
    "                        0, zhconv.convert(con, 'zh-cn')))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:09:47.487549Z",
     "start_time": "2020-06-12T11:09:06.782365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/trainingset_char_list.txt\n",
      "./datasets/validationset_char_list.txt\n",
      "./datasets/testa_char_list.txt\n"
     ]
    }
   ],
   "source": [
    "all_csv = [\n",
    "    './datasets/trainingset.csv',\n",
    "    './datasets/validationset.csv',\n",
    "    './datasets/testa.csv',\n",
    "]\n",
    "stop_word_set = get_stop_word_set(only_punctuation=True)\n",
    "save_char_path = './datasets/all_char_list.txt'\n",
    "for path in all_csv:\n",
    "    save_char_content(save_char_path, path, stop_word_set)\n",
    "    save_char_content_single(path, stop_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:23:18.753046Z",
     "start_time": "2020-06-12T11:09:55.731731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73594880.0\n",
      "6380\n"
     ]
    }
   ],
   "source": [
    "sentences = MySentence(save_char_path, [])\n",
    "model = Word2Vec(sentences,\n",
    "                 sg=1,\n",
    "                 size=100,\n",
    "                 compute_loss=True,\n",
    "                 window=10,\n",
    "                 workers=8,\n",
    "                 iter=15,\n",
    "                 min_count = 2)\n",
    "print(model.get_latest_training_loss())\n",
    "print(len(model.wv.vocab))\n",
    "model.save('./saved/char2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:23:45.188133Z",
     "start_time": "2020-06-12T11:23:45.182533Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:23:46.006872Z",
     "start_time": "2020-06-12T11:23:46.001185Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            data.append(line.strip())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:24:56.313507Z",
     "start_time": "2020-06-12T11:24:55.949181Z"
    }
   },
   "outputs": [],
   "source": [
    "train_content_ori = get_data(\n",
    "    './datasets/trainingset_words_list.txt',\n",
    ")\n",
    "val_content_ori = get_data(\n",
    "    './datasets/validationset_words_list.txt'\n",
    ")\n",
    "test_content_ori = get_data(\n",
    "    './datasets/testa_words_list.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:24:58.308383Z",
     "start_time": "2020-06-12T11:24:58.303097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105000 15000 15000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_content_ori), len(val_content_ori), len(test_content_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:25:01.733511Z",
     "start_time": "2020-06-12T11:25:01.685868Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:25:08.658779Z",
     "start_time": "2020-06-12T11:25:02.606610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_content_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:25:17.053647Z",
     "start_time": "2020-06-12T11:25:09.026391Z"
    }
   },
   "outputs": [],
   "source": [
    "train_content = vectorizer.transform(train_content_ori)\n",
    "val_content = vectorizer.transform(val_content_ori)\n",
    "test_content = vectorizer.transform(test_content_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:25:17.504749Z",
     "start_time": "2020-06-12T11:25:17.474585Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:25:31.066000Z",
     "start_time": "2020-06-12T11:25:17.920582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=80, n_iter=7,\n",
       "             random_state=2018, tol=0.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=20*4, n_iter=7, random_state=2018)\n",
    "svd.fit(train_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:25:50.896575Z",
     "start_time": "2020-06-12T11:25:50.328773Z"
    }
   },
   "outputs": [],
   "source": [
    "train_svd = svd.transform(train_content)\n",
    "val_svd = svd.transform(val_content)\n",
    "test_svd = svd.transform(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T11:26:24.466600Z",
     "start_time": "2020-06-12T11:26:24.430457Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix = 'svd_tfidf_80'\n",
    "np.save('./saved/%s_train' % prefix, train_svd)\n",
    "np.save('./saved/%s_val' % prefix, val_svd)\n",
    "np.save('./saved/%s_test' % prefix, test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
